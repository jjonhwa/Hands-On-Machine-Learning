{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chapter5. 서포트 벡터 머신.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM8fg3haQqfCOzqLoKRK1TW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jjonhwa/Hands-On-Machine-Learning/blob/main/Chapter5_%EC%84%9C%ED%8F%AC%ED%8A%B8_%EB%B2%A1%ED%84%B0_%EB%A8%B8%EC%8B%A0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ss0P1xvpVT8V"
      },
      "source": [
        "**SVM**\r\n",
        "1. 선형, 비선형, 회귀, 이상치 탐색에도 사용할 수 있는 다목적 머신러닝 모델\r\n",
        "2. 특히, 복잡한 분류 문제, 작거나 중간 크기의 데이터셋일 때 적합하다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8AOqMU72VvIO"
      },
      "source": [
        "# 5.1 선형 SVM 분류\r\n",
        "선형 분류 : 두 클래스를 나누고 있을 뿐만 아니라 제일 가까운 훈련 샘플로부터 가능한 한 멀리 떨어져 있는 직선을 구한다.(라지 마진 분류 : 가장 폭이 넓은 도로를 찾는다.)  \r\n",
        "\r\n",
        "**Note** : SVM은 스케일에 민감하다.(`StandardScaler`를 사용하여 스케일을 조정하도록 하자.)  \r\n",
        "* 하드 마진 분류\r\n",
        "1. 모든 샘플이 도로 바깥쪽에 올바르게 분류된 경우\r\n",
        "2. 데이터가 선형적으로 구분될 수 있어야 사용할 수 있음.\r\n",
        "3. 이상치에 민감함.\r\n",
        "\r\n",
        "* 소프트 마진 분류\r\n",
        "1. 도로 폭을 가능한 넓게 유지하면서 마진 오류를 작게한다.\r\n",
        "2. SVM모델에서 하이퍼 파라미터 `C`를 이용하여 폭을 조정할 수 있다.(`C`가 작을수록 폭이 넓어진다.)\r\n",
        "\r\n",
        "**Note** : SVM을 사용할 때 Overfitting이 됬을 경우 하이퍼 파라미터 `C`를 감소시켜 모델을 규제할 수 있다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AGpiJzNtYktm",
        "outputId": "de083251-4e92-41d2-9dcc-2807ec95174e"
      },
      "source": [
        "import numpy as np\r\n",
        "from sklearn import datasets\r\n",
        "from sklearn.pipeline import Pipeline\r\n",
        "from sklearn.preprocessing import StandardScaler\r\n",
        "from sklearn.svm import LinearSVC\r\n",
        "\r\n",
        "iris = datasets.load_iris()\r\n",
        "X = iris['data'][:, (2,3)]\r\n",
        "y = (iris['target'] == 2).astype(np.float64)\r\n",
        "\r\n",
        "svm_clf = Pipeline([\r\n",
        "      ('scaler', StandardScaler()),\r\n",
        "      ('linear_svc', LinearSVC(C = 1, loss = 'hinge')),\r\n",
        "])\r\n",
        "\r\n",
        "svm_clf.fit(X, y)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(memory=None,\n",
              "         steps=[('scaler',\n",
              "                 StandardScaler(copy=True, with_mean=True, with_std=True)),\n",
              "                ('linear_svc',\n",
              "                 LinearSVC(C=1, class_weight=None, dual=True,\n",
              "                           fit_intercept=True, intercept_scaling=1,\n",
              "                           loss='hinge', max_iter=1000, multi_class='ovr',\n",
              "                           penalty='l2', random_state=None, tol=0.0001,\n",
              "                           verbose=0))],\n",
              "         verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t7WJqsGMZDEz",
        "outputId": "a104392e-caff-4a73-c2ec-7993c8f8c4f2"
      },
      "source": [
        "svm_clf.predict([[5.5, 1.7]])"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kATz4to5YjKm"
      },
      "source": [
        "* 선형 SVM 모델을 사용하는 방법(분류)\r\n",
        "1. LinearSVC 사용\r\n",
        "2. SVC에서 `kernel = 'linear'`로 준다.\r\n",
        "3. SGDClassifier를 이용한다. (`SGDClassifier(loss = 'hinge', alpha = 1/(m*C))`를 이용한다.) (SGDClassifier의 경우 빠르게 수렴하진 않지만 데이터 셋이 아주 커서 메모리에 적재할 수 없거나 온라인 학습으로 분류 문제를 다룰 때 유용하다.)\r\n",
        "\r\n",
        "**Note** : LinearSVC의 경우 규제에 편향이 포함되어 있어서 평균을 뺴서 중앙에 맞도록 해주어야 한다. 그래서 StandardScaler를 써서 맞춰 주도록 하며 loss의 경우 'hinge'로 지정해주도록 한다. 또한, 특성이 샘플보다 적다면 `dual`매개변수를 False로 지정해주어야 한다.(duality(쌍대) 문제)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F2kuRKpLXsvr"
      },
      "source": [
        "# 5.2 비선형  SVM 분류\r\n",
        "비선형을 만드는 방법을 알아보도록 하고 기본적으로 Pipeline 에 다항 특성을 추가해서 구성하는 방법이 있다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "74guzb1XZNG6",
        "outputId": "2b574715-415f-49b4-f9a8-2bbc0444ac8a"
      },
      "source": [
        "from sklearn.datasets import make_moons\r\n",
        "from sklearn.pipeline import Pipeline\r\n",
        "from sklearn.preprocessing import PolynomialFeatures\r\n",
        "\r\n",
        "X, y = make_moons(n_samples = 100, noise = 0.15)\r\n",
        "polynomial_svm_clf = Pipeline([\r\n",
        "      ('poly_features', PolynomialFeatures(degree = 3)),\r\n",
        "      ('scaler', StandardScaler()),\r\n",
        "      ('svm_clf', LinearSVC(C = 10, loss = 'hinge'))\r\n",
        "])\r\n",
        "\r\n",
        "polynomial_svm_clf.fit(X, y)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(memory=None,\n",
              "         steps=[('poly_features',\n",
              "                 PolynomialFeatures(degree=3, include_bias=True,\n",
              "                                    interaction_only=False, order='C')),\n",
              "                ('scaler',\n",
              "                 StandardScaler(copy=True, with_mean=True, with_std=True)),\n",
              "                ('svm_clf',\n",
              "                 LinearSVC(C=10, class_weight=None, dual=True,\n",
              "                           fit_intercept=True, intercept_scaling=1,\n",
              "                           loss='hinge', max_iter=1000, multi_class='ovr',\n",
              "                           penalty='l2', random_state=None, tol=0.0001,\n",
              "                           verbose=0))],\n",
              "         verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5X1ISAy_YLQZ"
      },
      "source": [
        "## 5.2.1 다항식 커널\r\n",
        "SVM을 사용할 때 커널 트릭을 이용한다.\r\n",
        "* 커널 트릭(kernel trick) : 실제로는 특성을 추가하지 않으면서 다항식 특성을 많이 추가한 것과 같은 결과를 얻는 방법  \r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0mfCXKUUjd-2",
        "outputId": "40da56c0-6166-4623-be01-574325ebff67"
      },
      "source": [
        "from sklearn.svm import SVC\r\n",
        "poly_kernel_svm_clf = Pipeline([\r\n",
        "      ('scaler', StandardScaler()),\r\n",
        "      ('svm_clf', SVC(kernel = 'poly', degree = 3, coef0 = 1, C = 5))\r\n",
        "])\r\n",
        "\r\n",
        "poly_kernel_svm_clf.fit(X, y)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(memory=None,\n",
              "         steps=[('scaler',\n",
              "                 StandardScaler(copy=True, with_mean=True, with_std=True)),\n",
              "                ('svm_clf',\n",
              "                 SVC(C=5, break_ties=False, cache_size=200, class_weight=None,\n",
              "                     coef0=1, decision_function_shape='ovr', degree=3,\n",
              "                     gamma='scale', kernel='poly', max_iter=-1,\n",
              "                     probability=False, random_state=None, shrinking=True,\n",
              "                     tol=0.001, verbose=False))],\n",
              "         verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PWnOLmr4jtkI"
      },
      "source": [
        "위의 코드는 '3차 다항식 커널'을  사용해 SVM 분류기를 훈련시킨다.  \r\n",
        "\r\n",
        "* 모델이 과대적합이라면 다항식의 차수를 줄여야한다.  \r\n",
        "\r\n",
        "* 매개변수 `coef0`은 모델이 높은 차수와 낮은 차수에 얼마나 영향을 받을지를 조절한다.  \r\n",
        "\r\n",
        "**Note** : 적절한 하이퍼파라미터를 찾는 일반적인 방법은 GridSearch를 이용하는 것이다. 처음에는 Grid의 폭을 크게하여 빠르게 검색하고 그 후에는 최적 값을 찾기 위해 Grid를 세밀하게 검색하여 사용하도록 한다.\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T8yLtFUgkK_v"
      },
      "source": [
        "## 5.2.2 유사도 특성\r\n",
        "비선형 특성을 다루는 또 다른 기법은 각 샘플이 특정 랜드마크와 얼마나 닮았는지 측정하는 '유사도 함수'로 계산한 특성을 추가하는 것이다.\r\n",
        "\r\n",
        "* 유사도 함수 : 각 샘플에 대해 특정 랜드마크와의 유사도를 측정하는 함수  \r\n",
        "\r\n",
        "\r\n",
        "* 유사도 함수 예제 : 가우시안 방사 기저 함수(RBF, Radial Basis Function)\r\n",
        "1. $l$ : 랜드마크\r\n",
        "2. $\\gamma$ : 랜드마크에서 멀어질수록 0에 수렴하는 속도를 조절함\r\n",
        "  - $\\gamma$값이 클수록 가까운 샘플선호\r\n",
        "  - 과대적합 위험 커짐\r\n",
        "  - 0: 랜드마크에서 아주 멀리 떨어진 경우\r\n",
        "  - 1: 랜드마크와 같은 위치인 경우\r\n",
        "\r\n",
        "가우시안 RBF : $\\phi_\\gamma (x, l)$ = $exp(-\\gamma\\| x-l\\|^2)$\r\n",
        "\r\n",
        "* 유사도 함수 적용 장단점\r\n",
        "1. 각 샘플을 랜드마크로 지정 후 유사도 특성을 추가하면 (n개의 특성을 가진 m개의 샘플)에서 (m개의 특성을 가진 m개의 샘플)이 된다.\r\n",
        "2. 장점 : 차원이 커지면서 선형적으로 구분될 가능성이 높아짐.\r\n",
        "3. 단점 : 훈련 세트가 매우 클 경우 동일한 크기의 아주 많은 특성이 생성됨."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vK8uP_jModnD"
      },
      "source": [
        "## 5.2.3 가우시안 RBF 커널\r\n",
        "\r\n",
        "다항 특성 방식과 마찬가지로 유사도 특성 방식도 머신러닝 알고리즘에 유용하게 사용될 수 있다.  \r\n",
        "\r\n",
        "추가 특성을 모두 계산하려면 연산 비용이 많이 드는데 특히 훈련 세트가 클 경우 더 그렇다.  \r\n",
        "\r\n",
        "다음은 가우시안 RBF 커널을 사용한 SVC 모델이다 확인해보도록 하자.\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QDLCKE4hpMSx",
        "outputId": "63c3e6a6-5403-4d26-ac05-aa5951e37c1d"
      },
      "source": [
        "rbf_kernel_svm_clf = Pipeline([\r\n",
        "      ('scaler', StandardScaler()),\r\n",
        "      ('svm_clf', SVC(kernel = 'rbf', gamma = 5, C = 0.001))\r\n",
        "])\r\n",
        "rbf_kernel_svm_clf.fit(X, y)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(memory=None,\n",
              "         steps=[('scaler',\n",
              "                 StandardScaler(copy=True, with_mean=True, with_std=True)),\n",
              "                ('svm_clf',\n",
              "                 SVC(C=0.001, break_ties=False, cache_size=200,\n",
              "                     class_weight=None, coef0=0.0,\n",
              "                     decision_function_shape='ovr', degree=3, gamma=5,\n",
              "                     kernel='rbf', max_iter=-1, probability=False,\n",
              "                     random_state=None, shrinking=True, tol=0.001,\n",
              "                     verbose=False))],\n",
              "         verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "elJ3NXAEpY6j"
      },
      "source": [
        "1. $\\gamma$를 증가시키면 종 모양 그래프가 조ㅗㅂ아져서 각 샘플의 영향 범위가 작아진다.\r\n",
        "2. $\\gamma$가 작으면 넓은 종 모양 그래프를 만들며 샘플이 넒은 범위에 걸쳐 영향을 주므로 경계가 더 부드러워 진다.\r\n",
        "3. 즉, $\\gamma$는 규제의 역할을 한다.(모델이 Overfit일 경우 감소시켜야 한다.) \r\n",
        "\r\n",
        "**Note**\r\n",
        "* 여러 가지 커널 중 어떤 것을 사용할지 모를 때는 경험적으로 봤을 떄 **선형 커널**을 가장 먼저 시도해보도록 하자. \r\n",
        "* `LinearSVC`가 `SVC(kernel = 'linear'`보다 훨씬 바르다는 것을 기억하도록 하자(Training Set이 아주 크거나 특성 수가 많을 경우 더욱 그렇다). \r\n",
        "* Training Set이 너무 크지 않다면 가우시안 RBF 커널도 시도해볼만 하다. \r\n",
        "* 시간과 컴퓨팅 성능이 충분하다면 cross validation & grid search도 사용하여 다른 커널들도 더 시도해보도록 하자."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O71DedaAqZYs"
      },
      "source": [
        "## 5.2.4 계산 복잡도\r\n",
        "Linear SVC 파이썬 클래스틑 선형 SVM을 위한 최적화된 알고리즘을 구현한 liblinear 라이브러리를 기반으로 한다. liblinear 라이브러리는 kernel trick을 지원하지 않지만 훈련 샘플과 특성 수에 거의 선형적으로 늘어난다. 이 알고리즘의 훈련 시간 복잡도는  대략 mxn 정도이다.(SVC는 libsvm 라이브러리 기반)\r\n",
        "\r\n",
        "* SVC의 경우 훈련 시간의 복잡도는 보통 $m^2$ x $n$과 $m^3$ x $n$사이이다.  \r\n",
        "* **이는 샘플 수가 늘어날수록 엄청나게 느려진다는 것을 의미하며 이로 인해 SVC의 경우 복잡하지만 작거나 중간 규모의 훈련 세트를 사용할 떄 알맞다.**\r\n",
        "* 희소 특성(각 샘플에 0이 아닌 특성이 몇개 없는 경우)인 경우에 잘 확장된다.(알고리즘의 성능이 샘플이 가진 0이 아닌 특성의 평균 수에 거의 비례한다.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RNEwO7tfyY0p"
      },
      "source": [
        "# 5.3 SVM 회귀\r\n",
        "* SVM을 분류가 아니라 회귀에 적용하는 방법은 목표를 반대로 하는 것이다.\r\n",
        "* 제한된 마진 오류(도로 밖의 샘플) 안에서 도로 안에 가능한 한 많은 샘플이 들어가도록 학습한다.(회귀에서 도로의 폭은 epsilon으로 조절한다.)\r\n",
        "* 선형 SVM 회귀의 경우 마진 안에 훈련 샘플이 추가되어도 모델의 예측에 영향이 없어서 **$\\epsilon$에 민감하지 않다**라고 말한다.\r\n",
        "\r\n",
        "다음의 코드는 `LinearSVR`을 사용한 선형 SVM 회귀이다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wzR48v4OrBGi",
        "outputId": "e7a7d8e8-456f-4903-ebec-e5ffda5948c7"
      },
      "source": [
        "from sklearn.svm import LinearSVR\r\n",
        "\r\n",
        "svm_reg = LinearSVR(epsilon = 1.5)\r\n",
        "svm_reg.fit(X, y)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LinearSVR(C=1.0, dual=True, epsilon=1.5, fit_intercept=True,\n",
              "          intercept_scaling=1.0, loss='epsilon_insensitive', max_iter=1000,\n",
              "          random_state=None, tol=0.0001, verbose=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cGS4pD8lrHKO"
      },
      "source": [
        "비선형 회귀 작업의 경우 '커널 SVM 모델'을 사용한다.(여기서 역시 C로 규제를 정할 수 있다.)   \r\n",
        "다음은 `SVR`을 사용한 비선형 SVM 회귀이다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5NYPXYGOrfyZ",
        "outputId": "8606b3c8-678b-42fc-923f-706aaa2ef83b"
      },
      "source": [
        "from sklearn.svm import SVR\r\n",
        "\r\n",
        "svm_poly_reg = SVR(kernel = 'poly', degree = 2, C = 100, epsilon = 0.1)\r\n",
        "svm_poly_reg.fit(X, y)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SVR(C=100, cache_size=200, coef0=0.0, degree=2, epsilon=0.1, gamma='scale',\n",
              "    kernel='poly', max_iter=-1, shrinking=True, tol=0.001, verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tMGZSrVJrprG"
      },
      "source": [
        "LinearSVR은 LinearSVC와 마찬가지로 필요한 시간이 Training Set에 비례해서 선형적으로 늘어나며 SVR은 Training Set이 커질수록 훨씬 느려진다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ZvAxaf3r5WV"
      },
      "source": [
        ""
      ]
    }
  ]
}